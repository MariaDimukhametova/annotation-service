использование тематический модель в извлечение
однословный термин
cМ. А. нокель cН. В. лукашевич
мгу им. М. В. ломоносов, москва нивц мгу им. М. В. ломоносов, москва
mnokel@gmail.com louk_nat@mail.ru
аннотация
В статья представить результат экспериментовпоприменениютематическихмо-
делейкзадачеизвлеченияоднословныхтерминоввкачестветекстовыхколлекцийбыть взять подборка стать из электронный
банковский журнал на русский язык и
англоязычнаячастькорпусапараллельный
текст Europarl. эксперимент показывать, что использование тематический информациизначительноулучшаеткачество
извлеченияоднословныхтерминовнезависимый от предметный область и использовать язык.
ключевой слово
тематический модель, кластеризация, извлечение однословный термин
1 введение
извлечение термин из текст определённый
предметный область играть значительный роль во
многий прикладной задача, в первый очередь –
в разработка и пополнение различный терминологический ресурс, такой как тезаурус и онтология [35]. поскольку разработка такой ресурс вручную достаточно трудоёмкий, за последний
год было провести большой количество исследование по автоматизация данный процесс.
большинствосовременныхметодовизвлечение
терминовосновываютсянаиспользованииразличный статистический и лингвистический признак слово. основный цель при этом заключаться
в получение упорядочить список кандидат в
термин, в начало который находиться как можно больше слово, с больший вероятность являться термин. В некоторый работа было экспериментально установить, что использование машинный обучение для комбинирование
признак значительно улучшать результат извлечение термин по сравнение с метод, основать только на один какомтый признак,
поскольку тот или иной признак только частично отражать особенность поведение термин в
текст [17].на текущий момент традиционно используемыедляизвлечениятерминовстатистическиепри-
знак никак не отражать тот факт, что большинство термин относиться к тот или иной подтема
предметный область. поэтому мы было сделать предположение, что выделение такой подтема
в коллекция текст способный улучшить качество
автоматический извлечение термин.для проверка этого предположение в статья быть рассмотреть различный метод выделение подтема в
коллекция текст, который часто в литература
называться статистический тематический модель [4].
некоторыевидыстатистическихтематический
моделеймогутосновыватьсянатрадиционныхметодахавтоматическойкластеризациитекст[12].
впоследнеевремяпредложенывероятностныемеханизм выделение подтема в текстовый коллекция такой, как метод, основать на скрыть
распределениидирихло(LatentDirichletallocation
[4]), который собственно и были назвать тематический модель и в настоящий время интенсивно исследуться в рамка различный приложение автоматический обработка текст ( [12], [29],
[3]).
основный задача дать статья заключаться в
исследование возможность использование тематический информация для повышение качество
извлеченияоднословныхтерминовдляэтойцель
вначале в текстовый коллекция выделяться подтема, затем к ним применяться некоторый модификация хорошо известный признак, который
впоследствии использоваться вместе с другой статистический и лингвистический признак.
для того чтобы результат, представить
в статья, не зависеть ни от предметный область,
ни от язык, были взять два предметный область
и соответствующий текстовый коллекция: банковский предметный область и текст банковский тематика на русский язык и широкий предметный
область современный общественный жизнь европа и речь с заседание европарламент на английский язык. при этом эксперимент быть строиться следующий образ:
1. вначале статистический тематический модель быть исследовать с точка зрение задача
извлечение однословный термин с цель
выбор хороший;
2. затембудетосуществленосравнениепризнак, посчитать для хороший тематический
модель, с остальной признак с цель
определение вклад, который давать использование тематический модель в рассматривать задача.
2 близкий работа
запоследниегодыбылопредложеномногоразличный статистический и лингвистический признак слово, использовать для извлечение однословный термин из коллекция текст определённый предметный область ( [6], [1], [20], [10] и
др.).
всё предложить признак можно разделить
на следующий группа:
признак,основать начастотность словкандидат. К этой группа относиться, например, признак TFRIDF, предложить в
работа [6] и использовать модель пуассон
для предсказание терминологичность слово;
признак,использующиеконтрастный коллекция, т.е. коллекция более общий тематика. один из наиболее характерный представитель дать группа являться широко
использовать на практика признак относительный частотность [1], основать на
сравненииотносительныхчастотностейсел
в рассматривать и в контрастный текстовый коллекция;
контекстный признак, соединять в себеинформациюочастотностислов-кандидат с данные о контекст их употребление.
наиболее известный представитель этой
группа являться признак C-Value [20] и
NC-Value [10], учитывать частота встречаемость объемлющий словосочетание для
кандидат в термин.
однако ни один из предложить признак
не являться определять [25], и фактически из
текстовизвлекаетсядовольнобольшойсписоксловкандидат, который затем должный быть проанализировать и подтвердить эксперт по предметный область. важно поэтому дополнять список использовать признак, что позволить получать в начало список как можно больше слово, с
больший вероятность являться термин.3 статистический тематический модель
Новыепризнакислов-кандидат,которыевводиться в дать статья, использовать информация,
получать статистический тематический модель в исследовать текстовый коллекция.
статистический тематический модель (далее – тематический модель) коллекция текстовый
документовнаосновестатистическихметодовопределять, к какой подтема относиться каждый документ и какой слово образовать каждый подтема, представлять себя список часто встречаться рядом друг с друг слово, упорядочить по убывание степень принадлежность ему
[34]. так, в таблица 1 представить первый десять
слово, наиболее полно характеризовать три случайно выбрать подтема, выделить из русскоязычныхтекстовбанковскойтематикирассмат-
риваемыть коллекция.
подтема 1 подтема 2 подтема 3
банкнот обучение германия
офшорный студент франция
счётчик учебный евро
купюра вуз европейский
подделка семинар польша
обращение образование европа
номинал знание чехия
монета специалист италия
подлинность слушатель немецкий
поддельный учитель французский
таблица 1: пример подтема
втематическихмодель,какправило,использоваться модель мешок слово, в который каждый документ рассматриваться как набор встречаться в он слово. при этом перед выделение подтема
текстовый коллекция обычно подвергаться предобработка, выделять только значимый слово в
каждый документ. В частность, в данный исследование для русский язык были отобрать только существительное и прилагательное, а для английский – только существительное, поскольку
они покрывать больший часть термин.
на сегодняшний день разработать достаточно
много различный тематический модель. для выбор модель для исследование были проанализировать предыдущий работа, в который осуществляться сравнение модель с точка зрение различный практический приложение. так, в работа [29]
утверждаться, что каждый тематический модель
иметь свой сильный и слабый сторона.
сравнивать между себя метод NMF (неотрицательнойматричнойфакторизация)иLDA(латентный размещение дирихло), автор приходить к
вывод, что оба этот алгоритм давать похожий качество, хотя NMF и выдавать немного больше бессвязный подтема. В работа [12] утверждаться, что
традиционный тематический модель показывать
приемлемоекачествовыделенияподтема,ноиметь
множество ограничение. В частность они предполагать, что каждый документ иметь только один тематика. В действительность же документ
представлять себя, как правило, смесь подтема.
крометого,авторыотмечать,чтопараметрытрадиционный модель достаточно сложно настраивать. В то же время в работа подчёркиваться, что
более сложный модель (такой как LDA) необязательно дать хороший результат.
поскольку, как следовать из упомянуть выше
работа, среди тематический модель нет явный лидер и непонятно, какой качество они показать в
рассматривать задача извлечение однословный
термин, было решить выбрать несколько наиболее характерный представитель, который условно можно отнести либо к вероятностный, либо
к метод кластеризация текст, рассматривать с точка зрение тематический модель. каждый из выбрать модель будет рассмотреть в
следующий подраздел.
3.1 тематический модель, основать на
метод кластеризация текст
традиционный тематический модель, как правило, основываться на метод жёсткий кластеризация,рассматривающихкаждыйдокументкак
разредить вектор в пространство слово большой размерность [28]. после окончание работа
алгоритм кластеризация каждый получиться
кластер рассматриваться как один большой документ для вычисление вероятность входящий в
него слово по следующий формула:
P(wjt) =TF(wjt)P
wTF(wjt)(1)
гдеTF(wjt)– частотность слово wв кластер t.
В процесс кластеризация текстовый документ можно выделить следующий общий шаг:
1. предобработка документ (фильтрация
слово);
2. преобразование документ во внутренний
представление (в вектор слово);
3. расчёт расстояние между документ на
основа внутренний представление;
4. Кластеризациядокументовнаосноверассчитанногорасстоянияспомощьюодногоизал-
горитм.
для численный оценка расстояние между документ необходимый способ определение значимость каждый слово в обособление один документ относительно другой. для этого былипредложеныразличныесхемывзвешиванияотдельный слово, наиболее распространить из который
являться схема TFIDF [19], который также была
включить в данный исследование. В ней каждый
слово в документ ставиться в соответствие величина, вычислять по следующий формула:
TFIDF (wjd) = TF(wjd)max
0;logN DF(w)
DF(w)
(2)
гдеN– общий число документ в коллекция,
DF(w)– число документ в коллекция, в который встречаться слово w.
В следующий раздел быть описать выбранныенамиметодыпостроениятрадиционныхтема-
тический модель.
3.1.1 средний и сферический средний
алгоритм средний [18] начинать свою работа со случайный инициализация центр масса
каждый кластер. далее он итеративный повторять следующий шаг:
1. всё документ разбиваться на кластер в
соответствие с тем, какой из центр масса
оказаться близкий по выбрать метрика;
2. длякаждогополученногокластерапересчитываться центр масса.
В качество метрика близость между два документ исследовалиться следующий:
Евклидово расстояние ( K-Means ) [18]:
sim(A; B ) =sX
i(Ai Bi)2(3)
Косинусная мера близость (сферический средний – SPK-Means) . при этом все вектор, представлять документ, нормализоваться к единичный гиперсфера [33]:
sim(A; B ) =P
i(AiBi)
rP
iAirP
iBi(4)
3.1.2 иерархический агломеративный кластеризация
алгоритм иерархический агломеративный кластеризация [14] изначально рассматривать каждый документ как отдельный кластер. затем он
итеративный повторять следующий шаг:
1. находиться и объединяться в новый кластер
два наиболее близкий кластер;
2. пересчитываться расстояние между новый
кластер и весь остальной.
процесс повторяться до тот пора, пока не остаться задать число кластер.
вкачествеспособовопределениянаиболееблизкий кластер исследовалиться следующий наиболее распространить [14]:
Complete-link (“полный связывание”). наиболее близкий кластер – это кластер с малый максимальный парный расстояние между документ;
Single-link (“одиночный связывание”). наиболее близкий кластер – это кластер с малый минимальный парный расстояние между документ;
Average-link (“среднеесвязывание”).Этокомпромисс между два предыдущий способ. наиболее близкий кластер – это кластер с малый средний парный расстояние между документ.
3.1.3 неотрицательный матричный факторизация (NMF)
алгоритм,изначальноразработанныйдлить
уменьшенияразмерность,зарекомендовалсебядлить
решение задача кластеризация [32]. данный алгоритм осуществлять нечёткий кластеризация, который относить один и тот же документ к разный
кластер с разный вероятность.
принимаянавходенеотрицательнуюразреженный матрица V, который получаться записывание вектор, представлять документ, по
столбец, алгоритм искать такой матрица WиH
малый размерность, что VWHпо некоторый
метрика. В качество такой метрика исследовалиться
следующий [16]:
Евклидово расстояние ( NMF Euc ):
jjA Bjj2=X
i;j(Aij Bij)2(5)
Расстояние кульбакалейблер для неотрицательный матрица ( NMF KL):
D(AjjB ) =X
i;j(AijlogAij
Bij Aij+Bij)(6)
В результат работа алгоритм в матрица W
получаться распределение слово по кластер, а в
матрица H– распределение документ по кластер.Нормируясоответствующиевеличиныдля
каждый слово/документ, можно получить вероятность принадлежность этого слово/документ
кластер вероятностный тематический модель
вероятностныетематическиемоделипредставляюткаждыйдокументввидесмесиподтема,вкотора каждый подтема представлять себя некоторый вероятностный распределение над слово.
вероятностный модель порождать слово по следующий правило:
P(wjd) =X
tP(wjt)P(tjd) (7)
гдеP(tjd)иP(wjt)– распределение подтема по документ и слово по подтема, а P(wjd)– наблюдать распределение слово по документ.
порождение слово происходить следующий образ. для каждый документ dи для каждый
слово выбираться тема tиз распределение
P(tjd), и затем генерироваться слово wиз распределение P(wjt).
самый известный представитель дать
категорииявляютсяметодвероятностноголатентный семантический индексирование (PLSI) и латентный размещение дирихло (LDA).
3.2.1 PLSI
метод PLSI, также известный как PLSA, был
предложный в работа [13]. данный метод моделировать матрица V, в который обозначать число
вхождение слово wiв документ dj, получаться из модель с подтема:
P(wi; dj) =kX
t=1P(t)P(djjt)P(wijt)(8)
параметр модель настраиваться с помощь
максимизация правдоподобие наблюдать данныхизматрица M,тотмаксимизируяследующий
функционал:
X
i;jTF(wijdj) logP(wi; dj)!max (9)
поскольку в статья [7] теоретически обосновать, что алгоритм NMF, минимизировать расстояние кульбакалейблер и рассмотреть в прошлый раздел, эквивалентный алгоритм PLSA, в
данный исследование метод PLSA не рассматриваться отдельно.
3.2.2 LDA
метод латентный размещение дирихло был
предложный в работа [4]. LDA расширять модель
PLSI, добавлять туда априорный распределение параметр модель ( P(wjt)иP(tjd)), считать их распределённый по закон дирихло.
для настройка параметр модель необходимый
байесовский вывод. однако, поскольку он алгоритмически неразрешимый [4], исследовалиться следующий два применять на практика приближённый способ байесовский вывод:
55LDA VB – вариационный байесовский вывод, описать в статья [4];
LDA Gibbs – метод монтекарло с марковский цепь, использовать сэмплирование гиббс [27].
3.3 базовый тематический модель
В качество baseline была взять “тематический”
модель, который не выделять никакой подтема, а
просто рассматривать каждый документ как отдельно взять подтема. дать модель будет использоваться мы в эксперимент для сравнение с другой метод.
4 коллекция текст для эксперимент
в всех эксперимент, описывать в дать
статья, слово-кандидаты извлекаться из два различный коллекция:
Коллекция банковский русскоязычный текст (10422 документ, примерно 15.5 млн
слово),взятыхизразличныхэлектронныхбанковский журнал: аудитор, банк и технология, рбк и др.;
Английскаячастькорпусапараллельныхтекстов Europarl [8] из заседание европарламент(документ,примерномлнсел).
для подтверждение терминологичность словкандидат использоваться следующий “золотой
стандарт”:
для русский язык – тезаурус, разработать вручную для центральный банк российский федерация и включать в себя
порядкатермин,относящихсяксфера банковский активность, денежный политика и макроэкономика;
Дляанглийскогоязыка–официальныймногопрофильный тезаурус европейский союз Eurovoc [9], предназначить для ручногоиндексированиязаседанийЕвропарламен-
тот. он английский версия включать в себя
15161 термин.
при этом словокандидат считаться термин,
если оно содержаться в тезаурус.
всепризнакисловкандидатоврассчитываться
для 5000 самый частотный слово. В качество метрикиоценкикачествабылавыбранаСредняяТоч-
ность (AvP) [19], определять для множество D
всех словокандидат и его подмножество Dq
D, представлять действительно термин (тотподтверждённые тезаурус):
AvP (n) =1
jDqjX
1kjD qj0
@rk0
@1
kX
1ikri1
A1
A
(10)
гдеri= 1, если i-е словокандидат 2Dq, и
ri= иначе. даннаяформулаотражать тотфакт,
что чем больше термин сосредоточить в вершина итоговый список словокандидат, тем выше мера средний точность.
эксперимент проводиться с разный число
выделять подтема: 50, 100 и 150 соответственно. визуально результат получаться разный,
но на качество извлечение термин это никак не
отразиться. поэтому все дальнейший эксперимент проводиться с число подтема, равный 100.
5 выбор хороший тематический модель
как уже было сказать выше, вначале быть
представить результат эксперимент по определениюнаилучшейтематическоймоделидляэтый быть предложить и посчитать для каждый из
рассмотреть выше тематический модель некоторый модификация известный признак слово.
5.1 признак, использовать тематический
информация
основнойидеейвсехпризнак,использовать
получить c помощь какойлибо тематический
модель информация, являться тот факт, что в началесписк,образующихподтема,сбольшойвероятность находиться термин. для эксперимент мы предложить некоторый модификация известный признак (см. таблица 2). В таблица 2
использоваться следующий обозначение:
TF(w)– частотность слово w
DF(w)– документный частотность слово w
P(wjt)– условный вероятность принадлежность слово подтема t
k– число топик
5.2 результат эксперимент
втаблицахипредставленырезультатыэксперимент для исследовать русский и английский корпус соответственно.
как видно из привести выше таблица, хороший качество независимо от язык и предметный
область давать тематический модель NMF, минимизирующаярасстояниеКульбака-Лейблера.так,
хороший признак для оба язык являться
Term Score с 16% (соответственно 21%) прирост
признак форма ла
частотность (TF)P
tP(wjt)
TFIDF TF(
w)logk
DF(
w)
Domain Consensu
s (DC) [22]  P
t(P(wjt)logP(wjt))
Maximum
TF max
tP(wjt)
Term
Score (TS) [3]P
tTS(
wjt)
TS(
wjt) = P(wjt) logP(wjt)
(Q
tP(wjt))1
k
TS-IDF TS(
w)logk
DF(w)
Maximum
TS (MTS) max
tTS(
wjt)
таблица 2: признак, использовать тематический информация
модель TF TFIDF DC MTF TS TSIDF MTS
K-Means 33.3 25.5 32.734.435.7 28.7 34.3
SPK-Means 35.5 27.2 3533.936.3 30.1 33.6
Single-link 34.8 39.9 33.638.938.4 40.5 39
Comp-link 35.6 4134.539.238.4 41 39.5
Av
erage-link 35.8 40.7 34.539.5 3940.9 39.6
NMFEuc40.8 42.5 40.340.8 4243.1 41.9
NMF KL 42.3 40.3 37.547.1 48.9 42.9 47.9
LDA
VB35.8 42.7 32.842.842.5 45.1 46.5
LDA
Gibbs 37.7 38.4 3546.242.6 42.8 47.2
Baseline 3437.6 32.838.538.1 42 38.1
таблица 3: средний точность признак на русский корпус
Model TF TFIDF DC MTF TS TSIDF MTS
K-Means 29.3 32.3 28.930.330.1 31.8 30.4
SPK-Means 28.1 29.8 27.928.728.6 29.7 28.7
Single-link 30.3 38.9 29.837.336.5 38.8 39.9
Comp-link 31.1 39.6 30.437.234.6 38.9 39
Av
erage-link 30.5 38.9 29.937.135.4 38.3 39.3
NMFEuc34.4 31.6 32.341.143.7 31.6 40.5
NMF KL 33.3 37.7 31.244.3 44.4 37.3 44.1
LDA
VB32.3 30.3 30.537.136.3 30.3 38.5
LDA
Gibbs 35.2 41.8 33.342.637.8 43.7 43.5
Baseline 31.5 32.8 303633.6 35 36.7
таблица 4: средний точность признак на английский корпус
качестваотносительнолучшихпризнаковбазов
модель (TFIDF для русский корпус и Maximum
Term Score для английский корпус).
помимо вычисление средний точность отдельный признак было также осуществить их комбинирование для каждый исследовать тематический модель в отдельность с помощь метод логистический регрессия, реализовать в библиотека Weka [30]. при этом проводиться четырёхкратный кросспроверка, означать, что весь исходный выборка разбиваться случайный образ
на четыре равный неперескаяться часть, и каждый часть по очередь становиться контрольный
подвыборка, а обучение проводиться по остальной три. результат комбинирование признак для русский и английский корпус представить в таблица 5.
как видно из привести выше таблица, тот-модель средний точность
для ру
сский для анга
лийский
корп
са корп
са
Baseline 44.9 36.2
K-Means 36.2 33.7
SPK-Means 38.1 33.3
Single-link 42.1 41.4
Complete-link 41.9 41.3
Av
erage-link 42.7 41.3
NMF Euc 43.4 43.8
NMF KL 49.5 44.5
LDA
VB 46.1 36.7
LDA
Gibbs 47.9 44.4
таблица 5: средний точность комбинирование
признак, использовать тематический информация
матический модель NMF, минимизировать расстояние кульбакалейблер, снова давать хороший качество с 10% прирост для русский и с 23%
прирост для английский корпус относительно
базовый тематический модель.
такой образ, хороший тематический модель оказаться модель NMF, минимизировать
расстояние кульбакалейблер.
6 сравнение с другой признак
для изучение вклад тематический информация в задача автоматический извлечение однословный термин было решить сравнить результатыпредложенныхпризнак,использующихтематическуюинформация,состальнымистатистическимиилингвистическимипризнакамидляобо-
их исследовать корпус для 5000 самый частотный слово.
В качество признак, не использовать тематический информация, были взять характерный
представитель группа, описать в раздел 2.
6.1 признак, основать на частотность
признакиизданнойгруппыопираютсянапредположениеотом,чтотермин,какправило,встречаютсявколлекциигораздочащеостальныхсел.
В исследование были включить следующий признак:частотность, документный частотность,
TFIDF[19],TFRIDF [6],Domain Consensus [22].
6.2 признак, использовать контрастный
коллекция
для вычисление признак этой категория помимо целевой коллекция текст предметный областииспользоваласьконтрастнаяколлекциятек-
ст более общий тематика. для русский язык в
качество таковой была взять подборка из примерно 1 миллион новостной текст, а для английский – граммный статистика из британский
национальный корпус [5].
основный идея такой признак заключаться
в том, что частотность термин в целевой и контрастный коллекция существенно различаться.
В данный исследование рассматриваться следующий признак: относительный частотность [1],
релевантность [26],TFIDF [19] с вычисление
документный частотность по контрастный коллекция, Contrastive Weight [2],
Discriminative Weight [31],KF-IDF [15],Lexical
Cohesion [24] илогарифм правдоподобие [11].
6.3 контекстный признак
контекстный признак соединять в себе информация о частотность словокандидат с данные о контекст их употребление в коллекция.
В данный исследование рассматриваться следующий признак: C-Value [20],NC-Value, MNC-Value
[10],Token-LR, Token-FLR, Type-LR ,Type-FLR [21],
Sum3,Sum10,Sum50,Insideness [17].
6.4 прочий признак
В качество остальной признак, не использовать тематический информация, рассматриваться номер позиция первый вхождение в документ, тип словокандидат (существительное
или прилагательное), слово-кандидаты, начинаться с заглавный буква, и существительное в
именительный падеж (“подлежащее”) и слово из
контекстный окно с некоторый самый частотный предопределить термин [23].
кроме этого, также рассматриваться и комбинация данные признак с некоторый статистический величина (такой, как частотность в
целевой корпус). весь было взять 28 такой признак.
6.5 результат эксперимент
хороший признак каждый из упомянуть выше группа для русский и английский корпус
привести в таблица 6 и 7.
группа признак
ов хороший признак AvP
основать на TFRIDF 41.1
частотность
использовать логарифм 36.9
контрастный
коллекция правдоподобие
контекстный Sum3 37.4
тематический Term
Score 48.9
таблица 6: средний точность хороший признак
для русский корпус
как видно из привести выше таблица, независимо от язык и предметный область лучшимигруппа признак
ов хороший призна
ак
основать на TFRIDF для 38.5
частотность подлёж
ащий
использовать TFIDFдля 34.2
контрастный
коллекция подлёж
ащий
контекстный C-Value 31.3
тематический Term
Score 44.5
таблица 7: средний точность хороший признак
для английский корпус
индивидуальный признак оказаться тематический, превзойти остальной на 19% и 15% среднейточностидлярусскогоианглийскогокорпус
соответственно.
для оценка же вклад тематический признак в общий модель извлечение однословный термин мы сравнить модель извлечение, учитывать тематический признак (7 baseline признак и 7 признак, посчитать для хороший
тематический модель NMF KL), и модель, не использовать их. результат сравнение для оба
рассматривать корпус привести в табл. 8
(комбинирование признак осуществляться с помощь логистический регрессия из библиотека
Weka [30]).
корпус средний точность
без тематический С тематический
признак признак
русский 54.6 56.3
английский 50.4 51.4
таблица 8: результат сравнение модель с тематический признак и без них
мы считать, что дать результат, показать на два разный коллекция, подтверждать,
что тематический модель действительно вносить
дополнительный информация в процесс автоматический извлечение термин.
В заключение в таблица 9 представить первый 10 элемент из список извлечь словкандидат,полученныхспомощьюмодель,учитывать тематический признак (при этом термин выделить курсив).
7 заключение
встатьепредставленырезультатыэкспериментальный исследование возможность применение
тематическихмоделейдляулучшениякачестваавтоматическогоизвлеченияоднословныхтермин.
былиисследованыразличныетематическиемодель (как вероятностный, так и традиционный методыкластеризация)ипредложенынесколькомодификация известный признак для упорядочивание словокандидат по убывание их терминологичность. В качество текстовый коллекция бы58№русский
корпус английский
корпус
банковский Memb
er
2 банк Minute
3 го
д Amendment
4 рф Document
кредитный EU
налоговый President
7 кредит People
пенсионный Dire
ctive
9 средство Year
10 клиент Question
таблица 9: пример извлечь словкандидат
ливзятыдваразличныхкорпус:электронныебанковский статья на русский язык и речь с заседание европарламент на английский язык.
эксперимент показать, что независимо от
предметный область и язык использование тематическойинформацииспособнозначительноулуч-
шить качество автоматический извлечение однословный термин.
список литература
[1] K. Ahmad, L. Gillam, L. Tostevin. University
of Survey Participation in Trec8. Weirdness
indexing for logical document extrapolation and
retrieval.IntheProceedingsofTREC1999,1999.
[2] R.Basili,A.Moschitti,M.Pazienza,F.Zanzotto.
A Contrastive Approach to Term Extraction.
In the Proceedings of the 4th Terminology and
Artiﬁcial Intelligence Conference, 2001.
[3] D. Blei and J. Laﬀerty. Topic Models.
Text Mining: Classiﬁcation, Clustering and
Applications, Chapman & Hall, pp. 71–89, 2009.
[4] D. Blei, A. Ng and M. Jordan. Latent Dirichlet
Allocation. Journal of Machine Learning
Research, No 3, pp. 993–1022, 2003.
[5] British National Corpus. http://www.natcorp.
ox.ac.uk/
[6] K. Church and W. Gale. Inverse Document
Frequency IDF. A Measure of Deviation from
Poisson. In the Proceedings of the Third
Workshop on Very Large Corpora. MIT Press,
pp. 121–130, 1995.
[7] ChrisDing,TaoLi,WeiPeng.Ontheequivalence
between Non-negative Matrix Factorization
and Probabilistic Latent Semantic Indexing.
Computational Statistics and Data Analysis, No
52, pp. 3913–3927, 2008.
[8] European Parliament Proceedings Parallel
Corpus 1996–2011. http://www.statmt.org/
europarl/[9] EuroVoc. Multilingual Thesaurus of the
European Union. http://eurovoc.europa.
eu/drupal/
[10] K. Frantzi and S. Ananiadou. Automatic
Term Recognition Using Contextual Cues. In
the Proceedings of the IJCAI Workshop on
Computational Terminology, pp. 29–35, 2002.
[11] A. Gelbukh, G. Sidorov, E. Lavin-Villa,
L. Chanona-Hernandez. Automatic Term
Extraction using Log-likelihood based
Comparison with General Reference Corpora.
In the Proceedings of the Natural Language
Processing and Information Systems, pp.
248–255, 2010.
[12] Q. He, K. Chang, E. Lim, A. Banerjee.
Keep It Smile with Time: A Reeximanation
of Probabilistic Topic Detection Models. In
the Proceedings of IEEE Transactions Pattern
Analysis and Machine Intelligence. Volume 32,
Issue 10, pp. 1795–1808, 2010.
[13] Thomas Hofmann. Probabilistic Latent
Semantic Indexing. In the Proceedings of the
22nd Annual International SIGIR Conference
on Research and Development in Information
Retrieval, ACM New York, USA, pp. 50–57,
1999.
[14] S. C. Johnson. Hierarchical Clustering Schemes.
Psychometrica, No 2, pp. 241–254, 1967.
[15] D. Kurz and F. Xu. Text Mining for the
Extraction of Domain Retrieval Terms and
Term Collocations. In the Proceedings of
the International Workshop on Computational
Approaches to Collocations, 2002.
[16] Daniel D. Lee and H. Sebastian Seung.
Algorithms for Non-negative Matrix
Factorization. In the Proceedings of NIPS,
pp. 556–562, 2000.
[17] N. Loukachevitch. Automatic Term Recognition
Needs Multiple Evidence. In the Proceedings of
the 8th International Conference on LREC, 2012.
[18] J. B. MacQueen. Some Methods for
classiﬁcation and Analysis of Multivariate
Observations. In the Proceedings of the 5th
Berkeley Symposium on Mathematical Statistics
and Probability. University of California Press,
pp. 281–297, 1967.
[19] Christopher D. Manning, Prabhakar Raghavan
andHinrichSchutze.IntroductiontoInformation
Retrieval. Cambridge University Press, 2008.
[20] H. Nakagawa and T. Mori. A Simple but
Powerful Automatic Term Extraction Mehod.
59In the Proceedings of the Second International
Workshop on Computational Terminology, pp.
29–35, 2002.
[21] H. Nakagawa and T. Mori. Automatic Term
Recognition based on Statistics of Compound
Nouns and their Components. Terminology, vol.
9, no. 2, pp. 201–219, 2003.
[22] R. Navigli and P. Velardi. Semantic
Interpretation of Terminological Strings. In the
Proceedings of the 6th International Conference
on Terminology and Knowledge Engineering,
Springer, pp. 95–100, 2002.
[23] M. A. Nokel, E. I. Bolshakova, N. V.
Loukachevitch. Combining Multiple Features for
Single-Word Term Extraction. компьютерный
лингвистика и интеллектуальный технология.
по материал конференция диалог, бекасовый, pp. 490–501.
[24] Y. Park, R. J. Bird, B. Boguraev. Automatic
glossary extraction beyond terminology
identiﬁcation. In the Proceedings of the 19th
International Conference on Computational
Linguistics, 2002.
[25] P. Pecina and P. Schlesinger. Combining
Association Measures for Collocation Extraction.
In the Proceedings of the COLING/ACL, ACL
Press, pp. 651–658, 2006.
[26] A. Pe˜ nas, V. Verdejo, J. Gonzalo. Corbus-based
Terminology Extraction Applied to Information
Access. In the Proceedings of the Corpus
Linguistics 2001 Conference, pp. 458–465, 2001.
[27] X.-H. Phan, C.-T. Nguyen. GibbsLDA++:
A C/C++ implementation of latent Dirichlet
Allocation (LDA), 2007.
[28] G. Salton. Automatic text processing: the
transformation, analysis, and retrieval of
information by computer. Addison-Wesley, 1989.
[29] K. Stevens, P. Kegelmeyer, D. Andrzejewski, D.
Buttler. Exploring Topic Coherence over many
models and many topics. In the Proceedings of
EMNLP-CoNLL, pp. 952–961, 2012.
[30] Weka 3. Data Mining Software in Java. http:
//www.cs.waikato.ac.nz/ml/weka
[31] W.Wong,W.Liu,M.Bennamoun.Determining
Termhood for Learning Domain Ontologies
using Domain Prevalence and Tendency. In the
Proceedings of the 6th Australasian Conference
on Data Mining, pp. 47–54, 2007.
[32] W. Xu, X. Liu, Y. Gong. Document Clustering
Based On Non-negative Matrix Factorization. In
the Proceedings of SIRGIR, pp. 267–273, 2003.[33] Shi Zhong. Eﬃcient Online Spherical K-means
Clustering. In the Proceedings of IEEE-IJCNN,
Monreal, Canada, July 31 – August 4, pp. 3180–
3185, 2005.
[34] К. В. воронцов и А. А. потапенко. регуляризация, робастность и разрежённость вероятностный тематический модель. журнал
“компьютерный исследование и моделирование”, т. 4, №12, с. 693–706, 2012.
[35] Н. В. лукашевич. тезаурус в задача информационный поиск. москва: издательство
московский университет, 2011.
Application of Topic Models to the
Task of Single-Word Term Extraction
Michael Nokel, Natalia Loukachevitch
Thepaperdescribestheresultsofanexperimental
study of statistical topic models applied to the task
of single-word term extraction. The English part of
the Europarl corpus and the Russian articles taken
from online banking magazines were used as target
text collections. The experiments demonstrate that
topic information signiﬁcantly improves the quality
ofsingle-wordtermextraction,regardlessofthesubject
area and the language used.
60